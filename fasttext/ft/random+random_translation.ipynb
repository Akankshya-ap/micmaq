{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "class FastVector1:\n",
    "    \"\"\"\n",
    "    Minimal wrapper for fastvector embeddings.\n",
    "    ```\n",
    "    Usage:\n",
    "        $ model = FastVector(vector_file='/path/to/wiki.en.vec')\n",
    "        $ 'apple' in model\n",
    "        > TRUE\n",
    "        $ model['apple'].shape\n",
    "        > (300,)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_file='', transform=None):\n",
    "        \"\"\"Read in word vectors in fasttext format\"\"\"\n",
    "        self.word2id = {}\n",
    "\n",
    "        # Captures word order, for export() and translate methods\n",
    "        self.id2word = []\n",
    "\n",
    "        print('reading word vectors from %s' % vector_file)\n",
    "        with open(vector_file, 'r') as f:\n",
    "\t    print ('1') \n",
    "            (self.n_words, self.n_dim) = \\\n",
    "            (int(x) for x in f.readline().rstrip('\\n').split(' '))\n",
    "            self.embed = np.zeros((self.n_words, self.n_dim))\n",
    "            for i, line in enumerate(f):\n",
    "                elems = line.rstrip('\\n').split(' ')\n",
    "                self.word2id[elems[0]] = i\n",
    "                #print (elems[0])\n",
    "                self.embed[i] = elems[1:self.n_dim+1]\n",
    "                self.id2word.append(elems[0])\n",
    "        \n",
    "        # Used in translate_inverted_softmax()\n",
    "        self.softmax_denominators = None\n",
    "        \n",
    "        if transform is not None:\n",
    "            print('Applying transformation to embedding')\n",
    "            self.apply_transform(transform)\n",
    "    \n",
    "    def apply_cop(self, matrix,i):\n",
    "        self.embed[i]=matrix[:]\n",
    "    \n",
    "    def export(self, outpath):\n",
    "        \"\"\"\n",
    "        Transforming a large matrix of WordVectors is expensive. \n",
    "        This method lets you write the transformed matrix back to a file for future use\n",
    "        :param The path to the output file to be written \n",
    "        \"\"\"\n",
    "        fout = open(outpath, \"w\")\n",
    "\n",
    "        # Header takes the guesswork out of loading by recording how many lines, vector dims\n",
    "        fout.write(str(self.n_words) + \" \" + str(self.n_dim) + \"\\n\")\n",
    "        for token in self.id2word:\n",
    "            vector_components = [\"%.6f\" % number for number in self[token]]\n",
    "            vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "            out_line = token + \" \" + vector_as_string + \"\\n\"\n",
    "            fout.write(out_line)\n",
    "\n",
    "        fout.close()\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    \n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.word2id\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.embed[self.word2id[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastVector2:\n",
    "    \"\"\"\n",
    "    Minimal wrapper for fastvector embeddings.\n",
    "    ```\n",
    "    Usage:\n",
    "        $ model = FastVector(vector_file='/path/to/wiki.en.vec')\n",
    "        $ 'apple' in model\n",
    "        > TRUE\n",
    "        $ model['apple'].shape\n",
    "        > (300,)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_file='', transform=None):\n",
    "        \"\"\"Read in word vectors in fasttext format\"\"\"\n",
    "        self.word2id = {}\n",
    "\n",
    "        # Captures word order, for export() and translate methods\n",
    "        self.id2word = []\n",
    "\n",
    "        print('reading word vectors from %s' % vector_file)\n",
    "        with open(vector_file, 'r') as f:\n",
    "\t    print ('1') \n",
    "            (self.n_words, self.n_dim) = \\\n",
    "            (int(x) for x in f.readline().rstrip('\\n').split(' '))\n",
    "            self.embed = np.zeros((self.n_words, self.n_dim))\n",
    "            for i, line in enumerate(f):\n",
    "                elems = line.rstrip('\\n').split(' ')\n",
    "                self.word2id[elems[0]] = i\n",
    "                #print (elems[0])\n",
    "                ini=np.random.rand(300)\n",
    "                self.embed[i] = ini[:]\n",
    "                \n",
    "                self.id2word.append(elems[0])\n",
    "        \n",
    "        # Used in translate_inverted_softmax()\n",
    "        self.softmax_denominators = None\n",
    "        \n",
    "        if transform is not None:\n",
    "            print('Applying transformation to embedding')\n",
    "            self.apply_transform(transform)\n",
    "    \n",
    "    def apply_cop(self, matrix,i):\n",
    "        self.embed[i]=matrix[:]\n",
    "    \n",
    "    def export(self, outpath):\n",
    "        \"\"\"\n",
    "        Transforming a large matrix of WordVectors is expensive. \n",
    "        This method lets you write the transformed matrix back to a file for future use\n",
    "        :param The path to the output file to be written \n",
    "        \"\"\"\n",
    "        fout = open(outpath, \"w\")\n",
    "\n",
    "        # Header takes the guesswork out of loading by recording how many lines, vector dims\n",
    "        fout.write(str(self.n_words) + \" \" + str(self.n_dim) + \"\\n\")\n",
    "        for token in self.id2word:\n",
    "            vector_components = [\"%.6f\" % number for number in self[token]]\n",
    "            vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "            out_line = token + \" \" + vector_as_string + \"\\n\"\n",
    "            fout.write(out_line)\n",
    "\n",
    "        fout.close()\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    \n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.word2id\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.embed[self.word2id[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
    "    return np.dot(vec_a, vec_b) / \\\n",
    "        (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "    ti=[]\n",
    "    count=0\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        #print source,target\n",
    "        if source in source_dictionary.word2id and target in target_dictionary.word2id:\n",
    "            #print source, target\n",
    "            count=count+1\n",
    "            #print source, target\n",
    "            #print source+1\n",
    "            x=randint(0,len(source_dictionary.word2id))\n",
    "\n",
    "            #print x\n",
    "            source1= source_dictionary.id2word[x]\n",
    "            print source, source1,target\n",
    "            ti.append(target_dictionary.word2id[target])\n",
    "            source_matrix.append(source_dictionary[source1])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "        \n",
    "    # return training matrices\n",
    "    #print count\n",
    "    return np.array(source_matrix), np.array(target_matrix), np.array(ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from /home/apatra/fastText/fastText_multilingual-master/eng.vec\n",
      "1\n",
      "reading word vectors from /home/apatra/fastText/fastText_multilingual-master/model.vec\n",
      "1\n",
      "0.06786694143044197\n"
     ]
    }
   ],
   "source": [
    "en_dictionary = FastVector1(vector_file='/home/apatra/fastText/fastText_multilingual-master/eng.vec')\n",
    "mi_dictionary = FastVector2(vector_file='/home/apatra/fastText/fastText_multilingual-master/model.vec')\n",
    "\n",
    "en_vector = en_dictionary[\"one\"]\n",
    "mi_vector = mi_dictionary[\"newt\"]\n",
    "print(cosine_similarity(en_vector, mi_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from /home/apatra/fastText/fastText_multilingual-master/model.vec\n",
      "1\n",
      "0.027983757088689765\n"
     ]
    }
   ],
   "source": [
    "mi_dictionary = FastVector2(vector_file='/home/apatra/fastText/fastText_multilingual-master/model.vec')\n",
    "\n",
    "en_vector = en_dictionary[\"one\"]\n",
    "mi_vector = mi_dictionary[\"newt\"]\n",
    "print(cosine_similarity(en_vector, mi_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_words = set(mi_dictionary.word2id.keys())\n",
    "en_words = set(en_dictionary.word2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "bilingual_dictionary=[]\n",
    "with codecs.open('/home/apatra/fastText/fastText_multilingual-master/eng-mic','r','utf-8') as f:\n",
    "    for line in f:\n",
    "        eng, mic=line.split(', ')\n",
    "        #print eng\n",
    "        eng=eng.strip('\\\"')\n",
    "        #print eng\n",
    "        mic=mic.strip('\\\"')\n",
    "        mic=mic.replace('\\n','')\n",
    "        mic=mic.replace('\"','')\n",
    "        #print eng, mic\n",
    "        bilingual_dictionary.append((eng,mic))\n",
    "#print bilingual_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose polli megnatl\n",
      "I Anglo-Protestant nin\n",
      "aboard consummate teppit\n",
      "aboriginal name.If Lnu\n",
      "abstruse properti temig\n",
      "adequate auxillary tepiet\n",
      "adequate Emulsions tepiaq\n",
      "again blitz-heavy app\n",
      "alive Apoteker mimajit\n",
      "allow presi- ignmuatl\n",
      "almost reviewBest suel\n",
      "also 27-year-old elg\n",
      "also rainbow- jel\n",
      "always Varner apjiw\n",
      "and RECQ1 jel\n",
      "and SAWX aq\n",
      "another virtuais igtig\n",
      "arrive bvt pegising\n",
      "aside P-Cable gmetug\n",
      "asleep Grantsboro nepat\n",
      "at Horine eteg\n",
      "attached swinstall naspit\n",
      "attached Zdiar nasteg\n",
      "authority Faux-suede alsusuti\n",
      "aware MicroISV gejiatl\n",
      "ay amile amuj\n",
      "aye Laddi amuj\n",
      "battle Retarding matntimg\n",
      "battle unvoluntary matnaggewaqan\n",
      "bear returns muin\n",
      "beaver sex.The kopit\n",
      "because AMPerhaps muta\n",
      "before Blagg tmg\n",
      "before pupilage gesgmnaq\n",
      "beside Female-to-Male gmetug\n",
      "beside Chowning anapiw\n",
      "blacksmith weapons.Iran klaptan\n",
      "blaze BALCA gnugwaqan\n",
      "bleed oceanview maltewiaq\n",
      "bless gsm elapatoq\n",
      "boss join-the-dots alsusit\n",
      "boss Olexandr assusit\n",
      "both Gibraltar-based gitg\n",
      "build intra-domain eltoq\n",
      "but Roslin gatu\n",
      "Canada brain.One Ganata\n",
      "canoe Hratch kwitn\n",
      "caribou scamster qalipu\n",
      "chase olive getanatl\n",
      "city 2895 gjigan\n",
      "cliff Cuxhaven mtasoq\n",
      "cloud Batoi alug\n",
      "cod third-strongest peju\n",
      "completely iconophiles lpa\n",
      "confess SARDINE agnutg\n",
      "cry SEGMENTATION etltemit\n",
      "cry S-days atgitemit\n",
      "deep CABER temig\n",
      "deer Dups lentuk\n",
      "detest QFX masgelmatl\n",
      "direct al-Irshad assusit\n",
      "direct Intelliseek alsusit\n",
      "discuss surface.At agnutmajig\n",
      "discussion analoga agnutmaqan\n",
      "disobey BikeAbout elistuatl\n",
      "drink i-485 esamqwat\n",
      "eagle sundowning kitpu\n",
      "earn hamamelis eltoq\n",
      "eel Bookcliff katew\n",
      "eight drop-of ukmuljin\n",
      "ended Sahai gaqiaq\n",
      "endure PLUTO saputaqatg\n",
      "enjoys 10.12.2009 gesatg\n",
      "enmeshed non-deluxe nasteg\n",
      "enough silentness tepiet\n",
      "enough numbers.But tepiaq\n",
      "eventually KingYou glapis\n",
      "exactly beethoven assma\n",
      "exit Calabro tewiet\n",
      "extinct XRBs getmenejig\n",
      "far 4,308 gneg\n",
      "fight publisher matnaggewaqan\n",
      "finally babyOh glapis\n",
      "finished Cuartos gaqiaq\n",
      "first HitTestCore amgwes\n",
      "first Déby tmg\n",
      "floor Aydinli msaqtaqt\n",
      "food choice.2. mijipjewei\n",
      "forever MHz. iapjiw\n",
      "full-grown продаже gisigweg\n",
      "full-grown locusta gisigwet\n",
      "gather 50mil megnatl\n",
      "goes Romanies eliaq\n",
      "goes Naktong eliet\n",
      "ground inhumanities maqamigew\n",
      "handy T.H.E. tepaw\n",
      "happens cases.Click teliaq\n",
      "have Robertet geggung\n",
      "have caringly geggunatl\n",
      "he Chidozie telimatl\n",
      "he McGills negm\n",
      "hear deadline.If nutm\n",
      "her Burnand telimatl\n",
      "her MSAA negm\n",
      "here Birmania tet\n",
      "him 30.06.2003 negm\n",
      "hit Heliospheric taqamatl\n",
      "hundred 1917-1922 gasgiptnnaqan\n",
      "hundred thắng kaskimtlnaqn\n",
      "hunt MT8 getanatl\n",
      "immediately Ref2 smtug\n",
      "inch ARG2 mtijin\n",
      "instead Olthuis awna\n",
      "jab sculpteur taqamatl\n",
      "job ParkLives lugowaqan\n",
      "jobs talmud lugowaqan\n",
      "laborer 😠 lugowinu\n",
      "land Introna maqamigew\n",
      "law 30810 tplutaqan\n",
      "ledge -whom mtasoq\n",
      "leftover LVL10 esgwiet\n",
      "leftover houseOn esgwiaq\n",
      "life H33 mimajuaqan\n",
      "like fittest gesalatl\n",
      "like Perogative gesatg\n",
      "living Sleva mimajuaqan\n",
      "loath HETDEX masgelmatl\n",
      "located Beastwars eteg\n",
      "located Announces epit\n",
      "machine bk mulin\n",
      "make light-sabers eltoq\n",
      "man Cachette jinem\n",
      "marker Skeptics gnugwaqan\n",
      "mature micro-minis gisigweg\n",
      "mature ReplyDeleteBrenda gisigwet\n",
      "member glitterize naspit\n",
      "message Americans agnutmaqan\n",
      "mill opremljen mulin\n",
      "missionary 2004-12-13 aniapsuinu\n",
      "money لطفا suliewei\n",
      "month Gimli tepgunset\n",
      "Montreal overtaken Mulian\n",
      "moon E4000 tepgunset\n",
      "mud CanadaNext sisgu\n",
      "must ricevuta amujpa\n",
      "named 2-serving teluisig\n",
      "named Manucci teluisit\n",
      "native pataca Lnu\n",
      "near Ys gigjiw\n",
      "near 20x13 tepaw\n",
      "necessary theoretically-informed amujpa\n",
      "need SponsoredSterling menuegetoq\n",
      "negotiate 9-pound agnutmajig\n",
      "new ball-buster pilei\n",
      "news Beittel agnutmaqan\n",
      "news TranzAlpine glusuaqan\n",
      "night street-artist tekik\n",
      "night Godsends tepgig\n",
      "nighttime Caribbean-style tekik\n",
      "night-time Cankaya tekik\n",
      "nine Coprophagia pesqunatek\n",
      "offshore KEGS apaqt\n",
      "one biotrophs newt\n",
      "operates советский elugwet\n",
      "other ink.A igtig\n",
      "oversee Ebanks-Blake alsusit\n",
      "oversee those.My assusit\n",
      "pack 30-May-2012 ilajit\n",
      "path Nông awtij\n",
      "penitent 'come aniapsuinu\n",
      "perhaps lie etug\n",
      "permit Dkr ignmuatl\n",
      "person joyee mimajuinu\n",
      "pipe oil6 tmaqan\n",
      "placed Malvey eteg\n",
      "placed to.Although epit\n",
      "plan Microdeal ilsuteget\n",
      "possess Oyuncak geggunatl\n",
      "possess Zad geggung\n",
      "prepare thereany ilajit\n",
      "present Legguards eig\n",
      "profound BBPP temig\n",
      "promise impressions. teplumatl\n",
      "punch well-disposed taqamatl\n",
      "pyrexia Ghisallo epsimgewei\n",
      "reach Kotchman tepnatl\n",
      "reindeer postsAre qalipu\n",
      "remainder phones.- esgwiet\n",
      "remainder Wikinut esgwiaq\n",
      "repeat wilma app\n",
      "report Agonistes agnutmaqan\n",
      "request Bridge-based etawet\n",
      "require 2011Following menuegetoq\n",
      "rich bit.They milesit\n",
      "river IPAD2 sipu\n",
      "road Direct-Drive awti\n",
      "roast InsuranceHow etoqtasit\n",
      "salmon insect-inspired plamu\n",
      "says 26,190 teluet\n",
      "sea Variable-rate apaqt\n",
      "seated 0.294 epit\n",
      "see low-hung nemitoq\n",
      "see Sproles nemiatl\n",
      "see eHost nemitu\n",
      "seek koupil getanatl\n",
      "seldom Asakusabashi awisiw\n",
      "select CMAVE megnatl\n",
      "self-determination Day.by alsusuti\n",
      "send Elaho elgimatl\n",
      "she cloudflare-nginx negm\n",
      "should Feminazis etug\n",
      "sick COCKBURN gesnugwat\n",
      "silver deodex suliewei\n",
      "sing Ruderman etlintoq\n",
      "sitting 71km epit\n",
      "situated Форекс eteg\n",
      "six WRCB asukom\n",
      "six melinus asugom\n",
      "skunk Ëœ40 apikjilu\n",
      "slate NetZealous mtasoq\n",
      "slave retorno gisteju\n",
      "snake Conan-like mteskm\n",
      "so Mühendislik toqo\n",
      "some low-priced alt\n",
      "sometimes GPLSoftware jijuaqa\n",
      "soon 5571 apugjig\n",
      "speak DKKConvert gelusit\n",
      "spiteful Date19 getanatl\n",
      "sprout Αfter saqaliaq\n",
      "stand long.Since gaqamit\n",
      "story FacebookA agnutmaqan\n",
      "struggle Complimented matnaggewaqan\n",
      "suddenly Kyôko jiniw\n",
      "sufficient Colder tepiaq\n",
      "sufficient Wagashi tepiet\n",
      "supervise HMQ alsusit\n",
      "supervise KFGO assusit\n",
      "talk OFG etlewistoq\n",
      "talk NewsChristmas gelusit\n",
      "task KRTU lugowaqan\n",
      "tell noninstitutional telimatl\n",
      "ten 's.the mtln\n",
      "that NP18 ala\n",
      "that DH5alpha negla\n",
      "theirs drivers.My negmowowei\n",
      "them Jagirs negmow\n",
      "there TWiki ala\n",
      "there Briem eig\n",
      "they hip-belt negmow\n",
      "those Campings negla\n",
      "thousand аwesome pituimtlnaqn\n",
      "thumb Piascledine mtijin\n",
      "tired plunderphonics gispnet\n",
      "today thbe gisgug\n",
      "too Orsoni elg\n",
      "totally Cona lpa\n",
      "town podracer gutang\n",
      "track RECLAIMING getanatl\n",
      "trail J.System awti\n",
      "true lab.In teliaq\n",
      "two SteveD tapu\n",
      "us two-field ginu\n",
      "view 日曜日 angamatl\n",
      "war pramada matntimg\n",
      "we niceeeee ginu\n",
      "weep partick atgitemit\n",
      "what her.Today goqwei\n",
      "where Sheethal tami\n",
      "wolf CraigJune paqtesm\n",
      "woman Karam e'pit\n",
      "word Homecoming glusuaqan\n",
      "work black-lined elugwet\n",
      "work Mascagni lugowaqan\n",
      "worker Macpac lugowinu\n",
      "yeah NO.sub.2 amuj\n",
      "yep non-surviving amuj\n",
      "yes resort.The amuj\n",
      "you buller gilew\n",
      "fever Khris epsimgewei\n",
      "hate Wagen masgelmatl\n",
      "love 2007-01-04 gesatg\n",
      "love Audiocassettes gesalatl\n",
      "sleep 10,318 nepat\n",
      "water looner samqwan\n"
     ]
    }
   ],
   "source": [
    "# form the training matrices\n",
    "#from copy import deepcopy\n",
    "source_matrix, target_matrix ,ti= make_training_matrices(\n",
    "    en_dictionary, mi_dictionary, bilingual_dictionary)\n",
    "#print len(source_matrix), len(target_matrix)\n",
    "# learn and apply the transformation\n",
    "#print ti, len(ti)\n",
    "#target_matrix=deepcopy(source_matrix)\n",
    "#print source_matrix #[60][9], target_matrix[60][9]\n",
    "#transform = learn_transformation(source_matrix, target_matrix)\n",
    "#print type(transform)\n",
    "#print transform[299]\n",
    "#en_dictionary.apply_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nind=6\\np=list_duplicates(ti,ti[ind])\\n#for l in p:\\n #   print l\\nj=np.zeros(300)\\nprint source_matrix[ind]\\nprint source_matrix[191]\\nfor l in p:\\n    for x in l:\\n        j+=source_matrix[x]\\n            \\n    target_matrix[ind]=j[:]/len(l)\\nprint target_matrix[ind]\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def list_duplicates(seq, x):\n",
    "    tally = defaultdict(list)\n",
    "    for i,item in enumerate(seq):\n",
    "        tally[item].append(i)\n",
    "    return (locs for key,locs in tally.items() \n",
    "            if key==x)\n",
    "\n",
    "'''\n",
    "ind=6\n",
    "p=list_duplicates(ti,ti[ind])\n",
    "#for l in p:\n",
    " #   print l\n",
    "j=np.zeros(300)\n",
    "print source_matrix[ind]\n",
    "print source_matrix[191]\n",
    "for l in p:\n",
    "    for x in l:\n",
    "        j+=source_matrix[x]\n",
    "            \n",
    "    target_matrix[ind]=j[:]/len(l)\n",
    "print target_matrix[ind]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "count_no=0\n",
    "j=np.zeros(300)\n",
    "for r in range(0,len(ti)):\n",
    "    #print source_matrix[r], target_matrix[r]\n",
    "    #print len(source_matrix[r]),len(target_matrix[r])\n",
    "    #print ti[r]\n",
    "    p=list_duplicates(ti,ti[r])\n",
    "    #print p\n",
    "    j=np.zeros(300)\n",
    "    for l in p:\n",
    "        #print l\n",
    "        for x in l:\n",
    "            j+=source_matrix[x]\n",
    "            \n",
    "        target_matrix[r]=j[:]/len(l)\n",
    "    count_no+=1\n",
    "    #target_matrix[r]=source_matrix[r][:]\n",
    "    mi_dictionary.apply_cop(target_matrix[r],ti[r])\n",
    "#print count_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mi_dictionary.export('/home/apatra/Desktop/work/lstm/data/micmaq6.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_dictionary.export('/home/apatra/fastText/fastText_multilingual-master/micmaq6.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range (0,len(bilingual_dictionary)):\\n    print bilingual_dictionary[i]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in range (0,len(bilingual_dictionary)):\n",
    "    print bilingual_dictionary[i]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "x=randint(0,len(en_dictionary.word2id))\n",
    "\n",
    "print x\n",
    "print en_dictionary.id2word[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(en_dictionary.word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
