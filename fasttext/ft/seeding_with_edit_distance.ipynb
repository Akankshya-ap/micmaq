{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FastVector:\n",
    "    \"\"\"\n",
    "    Minimal wrapper for fastvector embeddings.\n",
    "    ```\n",
    "    Usage:\n",
    "        $ model = FastVector(vector_file='/path/to/wiki.en.vec')\n",
    "        $ 'apple' in model\n",
    "        > TRUE\n",
    "        $ model['apple'].shape\n",
    "        > (300,)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_file='', transform=None):\n",
    "        \"\"\"Read in word vectors in fasttext format\"\"\"\n",
    "        self.word2id = {}\n",
    "\n",
    "        # Captures word order, for export() and translate methods\n",
    "        self.id2word = []\n",
    "\n",
    "        print('reading word vectors from %s' % vector_file)\n",
    "        with open(vector_file, 'r') as f:\n",
    "\t    print ('1') \n",
    "            (self.n_words, self.n_dim) = \\\n",
    "            (int(x) for x in f.readline().rstrip('\\n').split(' '))\n",
    "            self.embed = np.zeros((self.n_words, self.n_dim))\n",
    "            for i, line in enumerate(f):\n",
    "                elems = line.rstrip('\\n').split(' ')\n",
    "                self.word2id[elems[0]] = i\n",
    "                #print (elems[0])\n",
    "                self.embed[i] = elems[1:self.n_dim+1]\n",
    "                self.id2word.append(elems[0])\n",
    "        \n",
    "        # Used in translate_inverted_softmax()\n",
    "        self.softmax_denominators = None\n",
    "        \n",
    "        if transform is not None:\n",
    "            print('Applying transformation to embedding')\n",
    "            self.apply_transform(transform)\n",
    "    \n",
    "    def apply_cop(self, matrix,i):\n",
    "        self.embed[i]=matrix[:]\n",
    "    \n",
    "    def export(self, outpath):\n",
    "        \"\"\"\n",
    "        Transforming a large matrix of WordVectors is expensive. \n",
    "        This method lets you write the transformed matrix back to a file for future use\n",
    "        :param The path to the output file to be written \n",
    "        \"\"\"\n",
    "        fout = open(outpath, \"w\")\n",
    "\n",
    "        # Header takes the guesswork out of loading by recording how many lines, vector dims\n",
    "        fout.write(str(self.n_words) + \" \" + str(self.n_dim) + \"\\n\")\n",
    "        for token in self.id2word:\n",
    "            vector_components = [\"%.6f\" % number for number in self[token]]\n",
    "            vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "            out_line = token + \" \" + vector_as_string + \"\\n\"\n",
    "            fout.write(out_line)\n",
    "\n",
    "        fout.close()\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def cosine_similarity(cls, vec_a, vec_b):\n",
    "        \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
    "        return np.dot(vec_a, vec_b) / \\\n",
    "            (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.word2id\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.embed[self.word2id[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def levenshteinDistance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "        #print s1,s2\n",
    "    distances = range(len(s1) + 1)\n",
    "    #print distances\n",
    "    #print enumerate(s2)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        #print i2,c2\n",
    "        distances_ = [i2+1]\n",
    "        #print distances_\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            #print distances_,i1, c1, c2\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "        #print distances, char\n",
    "        #if 0 not in distances and char=='@':\n",
    "            #char=c2\n",
    "\n",
    "        #if char in ['\\'','k','g','j']:\n",
    "            #dif=1\n",
    "        #print 'hi', distances[-1], char\n",
    "        #print char\n",
    "    return distances[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "    ti=[]\n",
    "    count=0\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        #print source,target\n",
    "        if source in source_dictionary and target in target_dictionary:\n",
    "            #print source, target\n",
    "            count=count+1\n",
    "            print source, target\n",
    "            ti.append(target_dictionary.word2id[target])\n",
    "            source_matrix.append(source_dictionary[source])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "        if source in source_dictionary and target not in target_dictionary:\n",
    "            for q in target_dictionary.word2id:\n",
    "                dis=levenshteinDistance(target,q)\n",
    "                if dis==1 :\n",
    "                    count=count+1\n",
    "                    print source, target,q\n",
    "                    ti.append(target_dictionary.word2id[q])\n",
    "                    source_matrix.append(source_dictionary[source])\n",
    "                    target_matrix.append(target_dictionary[q])\n",
    "                    #count+=1\n",
    "                    #print p, q\n",
    "    # return training matrices\n",
    "    print count\n",
    "    return np.array(source_matrix), np.array(target_matrix), np.array(ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from /home/apatra/fastText/fastText_multilingual-master/eng.vec\n",
      "1\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3ca4d6c9dc8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0men_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/apatra/fastText/fastText_multilingual-master/eng.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmi_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/apatra/fastText/fastText_multilingual-master/model.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0men_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"one\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmi_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmi_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"newt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2f66e208aa6f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vector_file, transform)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0melems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "en_dictionary = FastVector(vector_file='/home/apatra/fastText/fastText_multilingual-master/eng.vec')\n",
    "mi_dictionary = FastVector(vector_file='/home/apatra/fastText/fastText_multilingual-master/model.vec')\n",
    "\n",
    "en_vector = en_dictionary[\"one\"]\n",
    "mi_vector = mi_dictionary[\"newt\"]\n",
    "print(FastVector.cosine_similarity(en_vector, mi_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mi_dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-798fe28092a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmi_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0men_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mi_dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "mi_words = set(mi_dictionary.word2id.keys())\n",
    "en_words = set(en_dictionary.word2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "bilingual_dictionary=[]\n",
    "with codecs.open('/home/apatra/fastText/fastText_multilingual-master/eng-mic','r','utf-8') as f:\n",
    "    for line in f:\n",
    "        eng, mic=line.split(', ')\n",
    "        #print eng\n",
    "        eng=eng.strip('\\\"')\n",
    "        #print eng\n",
    "        mic=mic.strip('\\\"')\n",
    "        mic=mic.replace('\\n','')\n",
    "        mic=mic.replace('\"','')\n",
    "        #print eng, mic\n",
    "        bilingual_dictionary.append((eng,mic))\n",
    "#print bilingual_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# form the training matrices\n",
    "#from copy import deepcopy\n",
    "source_matrix, target_matrix ,ti= make_training_matrices(\n",
    "    en_dictionary, mi_dictionary, bilingual_dictionary)\n",
    "print len(source_matrix), len(target_matrix)\n",
    "# learn and apply the transformation\n",
    "print ti\n",
    "#target_matrix=deepcopy(source_matrix)\n",
    "print source_matrix[60][9], target_matrix[60][9]\n",
    "#transform = learn_transformation(source_matrix, target_matrix)\n",
    "#print type(transform)\n",
    "#print transform[299]\n",
    "#en_dictionary.apply_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def list_duplicates(seq, x):\n",
    "    tally = defaultdict(list)\n",
    "    for i,item in enumerate(seq):\n",
    "        tally[item].append(i)\n",
    "    return (locs for key,locs in tally.items() \n",
    "            if key==x)\n",
    "\n",
    "'''\n",
    "ind=6\n",
    "p=list_duplicates(ti,ti[ind])\n",
    "#for l in p:\n",
    " #   print l\n",
    "j=np.zeros(300)\n",
    "print source_matrix[ind]\n",
    "print source_matrix[191]\n",
    "for l in p:\n",
    "    for x in l:\n",
    "        j+=source_matrix[x]\n",
    "            \n",
    "    target_matrix[ind]=j[:]/len(l)\n",
    "print target_matrix[ind]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "count_no=0\n",
    "j=np.zeros(300)\n",
    "for r in range(0,len(ti)):\n",
    "    #print source_matrix[r], target_matrix[r]\n",
    "    #print len(source_matrix[r]),len(target_matrix[r])\n",
    "    p=list_duplicates(ti,ti[r])\n",
    "    j=np.zeros(300)\n",
    "    for l in p:\n",
    "        for x in l:\n",
    "            j+=source_matrix[x]\n",
    "            \n",
    "        target_matrix[r]=j[:]/len(l)\n",
    "    count_no+=1\n",
    "    #target_matrix[r]=source_matrix[r][:]\n",
    "    mi_dictionary.apply_cop(target_matrix[r],ti[r])\n",
    "print count_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_dictionary.export('/home/apatra/fastText/fastText_multilingual-master/micmaq3.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
